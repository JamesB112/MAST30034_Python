{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 Overview\n",
    "## First Half\n",
    "- Machine Learning - Discussion.\n",
    "\n",
    "\n",
    "## Second Half\n",
    "Advanced Content:\n",
    "- Introduction to PySpark continued.\n",
    "- Introduction to Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf\n",
    "- Generalize the model (train dataset will almost never be the same as \"test\" dataset, let alone real world test data)\n",
    "    - Avoid overfitting\n",
    "    - Remember the bias that can occur with a normal train-test split (80% train vs 20% test is not representitive and can change depending on the split)\n",
    "    - Cross validation\n",
    "- There is never enough data\n",
    "    - Models will always make certain assumptions \n",
    "    - Even the best model may be no better than a DummyClassifier (random predictions). Think about real world problems with 25% accuracy such as NLP...\n",
    "- Overfitting\n",
    "    - Bias: a learner's tendency to consistently learn the wrong labels\n",
    "    - Variance: tendency to learn random things irrespective of the true labels\n",
    "    ![bv](./cloud/bv.PNG)\n",
    "    - Consider bias on linear models. They rely on the data being linearly seperable by a hyperplane. But what if it is not...\n",
    "    - Consier variance on tree models.  They can represent any boolean function to seperate the data, but can learn very different things depending on the training data\n",
    "    - Can do cross validation or maybe a $\\chi^2$ test (from statistics / lsm) during feature selection \n",
    "- Curse of dimensionality\n",
    "    - As your feature space increases, the number of possible configurations grows exponentially, thus, the number of possible configurations covered by your observation decreases. \n",
    "    - Imagine a dataset with more features than observations\n",
    "    - https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e\n",
    "- Feature Engineering is Key\n",
    "    - Why do models fail? Most likely due to the feature space used.\n",
    "    - You will come to understand that all the time in ML is spent on preprocessing and feature engineering.\n",
    "    - A simple model that is interpretable and utilises all the data is far more favourable than a complex model with cannot be interpreted easily with a subset of the data...\n",
    "- Learn several models\n",
    "    - Use a baseline to figure out the difficulty of your problem\n",
    "    - Try a wide range of models (regression vs trees vs svm vs clustering) and analyse why some may do better than other\n",
    "    - Think of techniques such as bagging, boosting, etc\n",
    "        - Bagging: Generate random variations of the training set through resampling then use a set of classifiers in a voting system\n",
    "        - Boosting: Training examples have weights which are varied so each new classifier aim to focus on the examples that previous classifiers failed \n",
    "- Correlation does not imply causation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
